import sys
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json,col
import logging
import os
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType
logging.basicConfig(level=logging.INFO, # má»©c Ä‘á»™ ghi log
                    format='%(asctime)s:%(funcName)s:%(levelname)s:%(message)s', # Ä‘á»‹nh dáº¡ng log thá»i gian , tÃªn hÃ m, má»©c Ä‘á»™ log vÃ  thÃ´ng Ä‘iá»‡p
                    datefmt='%Y-%m-%d %H:%M:%S', # Ä‘á»‹nh dáº¡ng thá»i gian
                    filename='streaming.log', # ghi log vÃ o file
                    filemode='a', # cháº¿ Ä‘á»™ ghi Ä‘Ã¨ vÃ o file
                    encoding='utf-8' # mÃ£ hÃ³a utf-8
)
logger = logging.getLogger(__name__) # táº¡o logger vá»›i tÃªn cá»§a module hiá»‡n táº¡i, logname = stream1.py
from dotenv import load_dotenv
load_dotenv()

def create_spark_session():
    try:
        # ğŸ“¥ Láº¥y biáº¿n tá»« .env
        app_name = os.getenv("APP_NAME")
        cassandra_host = os.getenv("CASSANDRA_HOST")
        cassandra_port = os.getenv("CASSANDRA_PORT")
        cassandra_user = os.getenv("CASSANDRA_USER")
        cassandra_password = os.getenv("CASSANDRA_PASSWORD")
        cassandra_version = os.getenv("CASSANDRA_CONNECTOR_VERSION")
        kafka_version = os.getenv("KAFKA_CONNECTOR_VERSION")
        scala_version = os.getenv("SCALA_VERSION")

        # ğŸ§© GhÃ©p chuá»—i jar
        jars = ",".join([
            f"com.datastax.spark:spark-cassandra-connector_{scala_version}:{cassandra_version}",
            f"org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{kafka_version}"
        ])

        # ğŸš€ Táº¡o SparkSession
        spark = SparkSession.builder \
            .appName(app_name) \
            .config("spark.jars.packages", jars) \
            .config("spark.cassandra.connection.host", cassandra_host) \
            .config("spark.cassandra.connection.port", cassandra_port) \
            .config("spark.cassandra.auth.username", cassandra_user) \
            .config("spark.cassandra.auth.password", cassandra_password) \
            .getOrCreate()

        spark.sparkContext.setLogLevel("ERROR")
        logging.info("âœ… Spark session created successfully.")
        return spark

    except Exception as e:
        logging.error(f"âŒ Couldn't create Spark session: {e}")
        return None
def create_initial_dataframe(spark_session):
    try:
        # ğŸ“¥ Láº¥y biáº¿n tá»« .env
        kafka_bootstrap_servers = os.getenv("KAFKA_BOOTSTRAP_SERVERS")
        kafka_topic = os.getenv("KAFKA_TOPIC")

        # ğŸ§© Äá»‹nh nghÄ©a schema
        schema = StructType([
            StructField("Ngay", StringType(), True),
            StructField("GiaDieuChinh", DoubleType(), True),
            StructField("GiaDongCua", DoubleType(), True),
            StructField("ThayDoi", StringType(), True),  # dáº¡ng chuá»—i vÃ¬ cÃ³ dáº¥u %
            StructField("KhoiLuongKhopLenh", LongType(), True),
            StructField("GiaTriKhopLenh", LongType(), True),
            StructField("KLThoaThuan", LongType(), True),
            StructField("GtThoaThuan", LongType(), True),
            StructField("GiaMoCua", DoubleType(), True),
            StructField("GiaCaoNhat", DoubleType(), True),
            StructField("GiaThapNhat", DoubleType(), True),
            StructField("code", StringType(), True)
        ])

        # ğŸš€ Táº¡o DataFrame tá»« Kafka
        df = spark_session.readStream \
            .format("kafka") \
            .option("kafka.bootstrap.servers", kafka_bootstrap_servers) \
            .option("subscribe", kafka_topic) \
            .option("startingOffsets", "earliest") \
            .load()
        
        # ğŸ§© Chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u tá»« Kafka
        df = df.selectExpr("CAST(value AS STRING)") \
            .select(from_json(col("value"), schema).alias("data")) \
            .select("data.*")
        # ğŸ“œ Giáº£i thÃ­ch:
        """
        - `from_json` dÃ¹ng Ä‘á»ƒ phÃ¢n tÃ­ch cÃº phÃ¡p chuá»—i JSON thÃ nh cÃ¡c cá»™t
        vd : {"name": "TrÃ­", "age": 18} ,
        data = StructType([
            StructField("name", StringType()),
            StructField("age", IntegerType()    )
        ])
        - `col("value")` láº¥y cá»™t `value` tá»« DataFrame Kafka
        - `alias("data")` Ä‘áº·t tÃªn cho cá»™t má»›i lÃ  `data`
        - `select("data.*")` chá»n táº¥t cáº£ cÃ¡c cá»™t tá»« cá»™t `data`
        - `selectExpr("CAST(value AS STRING)")` chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u tá»« Kafka sang chuá»—i trong Ä‘Ã³ CAST lÃ  hÃ m chuyá»ƒn Ä‘á»•i kiá»ƒu dá»¯ liá»‡u
        - code Ä‘áº£m báº£o ráº±ng dá»¯ liá»‡u tá»« Kafka Ä‘Æ°á»£c chuyá»ƒn Ä‘á»•i thÃ nh DataFrame vá»›i cÃ¡c cá»™t Ä‘Ã£ Ä‘á»‹nh nghÄ©a trong schema.
        """
        # # ğŸ§© Äáº·t tÃªn cá»™t
        # df = df.toDF("id", "name", "age", "city")
    
        # ğŸš€ Hiá»ƒn thá»‹ thÃ´ng tin DataFrame
        df.printSchema()
        logging.info(f"âœ… Initial streaming DataFrame schema created successfully.")
        return df

    except Exception as e:
        logging.error(f"âŒ Couldn't create initial DataFrame: {e}")
        return None
def start_streaming(df):
    # ğŸš€ Báº¯t Ä‘áº§u streaming
    """    - `writeStream` lÃ  phÆ°Æ¡ng thá»©c Ä‘á»ƒ ghi dá»¯ liá»‡u streaming
    - `format("org.apache.spark.sql.cassandra")` chá»‰ Ä‘á»‹nh Ä‘á»‹nh dáº¡ng ghi lÃ  Cassandra
    - `outputMode("append")` chá»‰ Ä‘á»‹nh cháº¿ Ä‘á»™ ghi lÃ  thÃªm má»›i
    - `options(table="random_names", keyspace="spark_streaming")` chá»‰ Ä‘á»‹nh báº£ng vÃ  keyspace trong Cassandra
    - `start()` báº¯t Ä‘áº§u quÃ¡ trÃ¬nh ghi dá»¯ liá»‡u streaming
    """
    my_query = (df.writeStream
                .format("org.apache.spark.sql.cassandra")
                .outputMode("append")
                .options(table="random_names", keyspace="spark_streaming")\
                .start())

    return my_query.awaitTermination()
def write_streaming_data():
    spark = create_spark_session()
    df = create_initial_dataframe(spark)
    if df is None:
        logging.error("âŒ Failed to create initial DataFrame. Exiting.")
        return
    start_streaming(df)


if __name__ == '__main__':
    write_streaming_data()